{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7012849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import itertools\n",
    "from astropy.stats import sigma_clip\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9372ea96",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad3bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"/kaggle/input/ariel-data-challenge-2025/\"\n",
    "train_dir_name = \"/kaggle/input/ariel-data-challenge-2025/train/\"\n",
    "star_0_dir = f\"{train_dir_name}1010375142\"\n",
    "path_out = '/kaggle/working/'   # path to store the signal data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2f21a3",
   "metadata": {},
   "source": [
    "Analogue to digital conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ace43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adc_airs(signal, gain = float(adc_df[\"AIRS-CH0_adc_gain\"].iloc[0]), offset = float(adc_df[\"AIRS-CH0_adc_offset\"].iloc[0])):\n",
    "\n",
    "    signal = signal.astype(np.float64)\n",
    "    signal /= gain\n",
    "    signal += offset\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d110e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adc_fgs(signal, gain = float(adc_df[\"FGS1_adc_gain\"].iloc[0]), offset = float(adc_df[\"FGS1_adc_offset\"].iloc[0])):\n",
    "\n",
    "    signal = signal.astype(np.float64)\n",
    "    signal /= gain\n",
    "    signal += offset\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d692f0cc",
   "metadata": {},
   "source": [
    "### Calibration and binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bff555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking_dead_hot(signal, dead, dark):\n",
    "    '''\n",
    "    Mask dead and dark pixels from the signals :\\n\n",
    "    \n",
    "    '''\n",
    "    hot = sigma_clip(              # clipping values over upper 5 sigma and lower 5 sigma for the dark calibration dataset\n",
    "        data = dark, sigma = 5, maxiters = 5\n",
    "    )     # returns boolean array \n",
    "\n",
    "    hot = np.tile(hot, (signal.shape[0], 1, 1))   # repeating the boolean array for all the timestamps of the signal dataset\n",
    "    dead = np.tile(dead, (signal.shape[0], 1, 1))\n",
    "\n",
    "    signal = np.ma.masked_where(dead, signal)   # masking the signal dataset using the boolean dead and dark arrays\n",
    "    signal = np.ma.masked_where(hot, signal)\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da1b30",
   "metadata": {},
   "source": [
    "Dark current subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83046b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dark_current_sub(signal, dead, dark, dt):   # subtracting the dark current noise from the signals\n",
    "\n",
    "    dark = np.ma.masked_where(dead, dark)    # correcting the dark current map for the dead pixels\n",
    "    dark = np.tile(dark, (signal.shape[0],1,1))\n",
    "\n",
    "    signal -= dark * dt[:, np.newaxis, np.newaxis]   # expanding the dimensionality of the time integration \n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d21df",
   "metadata": {},
   "source": [
    "Non-Linearity correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03cf110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lin_corr(lin_corr, clean_signal):\n",
    "    lin_corr = np.flip(lin_corr, axis = 0)   # flipping the order of 0th axis of the dataframe to maintain the coefficients in descending powers \n",
    "\n",
    "    for x,y in itertools.product(     # looping through the index of each pixel in the image for a given time stamp \n",
    "        range(clean_signal.shape[1]), range(clean_signal.shape[2])\n",
    "    ):\n",
    "        poli = np.poly1d(lin_corr[:, x ,y])    # creating the polynomial function \n",
    "        clean_signal[: ,x,y] = poli(clean_signal[:,x,y])     # fitting the polynomial function on the input signal\n",
    "    return clean_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99a3c7",
   "metadata": {},
   "source": [
    "Flat Field correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59a0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_flat_field(flat,dead, signal):\n",
    "    \n",
    "    flat = flat.transpose(1, 0)\n",
    "    dead = dead.transpose(1, 0)\n",
    "    \n",
    "    flat = np.ma.masked_where(dead, flat) # masking the dead pixels\n",
    "    flat = np.tile(flat, (signal.shape[0], 1, 1))   # repeating the 2D array for all the time stamps \n",
    "    signal = signal / flat  \n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0254d",
   "metadata": {},
   "source": [
    "Getting correlated double sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ff0eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cds(signal):\n",
    "    cds = signal[:,1::2,:,:] - signal[:,::2,:,:]  # start exposure signal - end exposure signal \n",
    "    return cds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023bead7",
   "metadata": {},
   "source": [
    "Binning Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2b03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_obs(binning, cds_signals):\n",
    "    cds_transposed = cds_signals.transpose(0,1,3,2)    # transposing the x,y frame axises in the signal array\n",
    "    # creating a zeros array with binned time axis\n",
    "    cds_binned = np.zeros((cds_transposed.shape[0], cds_transposed.shape[1]//binning, cds_transposed.shape[2], cds_transposed.shape[3]))\n",
    "\n",
    "    # inserting values in the binned zeros array\n",
    "    for i in range(cds_transposed.shape[1]//binning):\n",
    "        cds_binned[:,i,:,:] = np.sum(cds_transposed[:,i * binning:(i + 1) * binning,:,:], axis = 1)\n",
    "\n",
    "    return cds_binned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01276d81",
   "metadata": {},
   "source": [
    "Getting the index of the training data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c109c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(files, chunk_size):\n",
    "    index = []\n",
    "\n",
    "    for file in files:\n",
    "        file_name = file.split(\"/\")[-1]   # getting the file name \n",
    "        # making sure we are working with AIRS parquet file for the star \n",
    "        if file_name.split('_')[0] == 'AIRS-CH0' and file_name.split('_')[1] == 'signal' and file_name.split('_')[2] == '0.parquet':\n",
    "            file_index = os.path.basename(os.path.dirname(file))  # going one directory up and extracting the basename for the path\n",
    "            index.append(int(file_index)) \n",
    "    index = np.array(index)   \n",
    "    index = np.sort(index)    # sorting the indices \n",
    "\n",
    "    index=np.array_split(index, len(index)//chunk_size)   # dividing the data indices into chunks \n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69ace8",
   "metadata": {},
   "source": [
    "Calibrating all the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(dir_name + 'train/', '*/*'))   # search subdirectories for all the files\n",
    "CHUNKS_SIZE = 10\n",
    "index = get_index(files,CHUNKS_SIZE)   # getting the index of the training data into chunks of given chunk size\n",
    "\n",
    "axis_info = pd.read_parquet(os.path.join(dir_name,'axis_info.parquet'))\n",
    "DO_MASK = True\n",
    "DO_THE_NL_CORR = False\n",
    "DO_DARK = True\n",
    "DO_FLAT = True\n",
    "TIME_BINNING = True\n",
    "\n",
    "# cutting the wavelength pixel axis to match last targets' points in the AIRS\n",
    "\n",
    "cut_inf, cut_sup = 39, 321\n",
    "l = cut_sup - cut_inf   \n",
    "\n",
    "# keeping the track of indices n using enumerate and initializing a progress bar \n",
    "# looping through the chunks in the index list (index is the list of all the chunks), n is the index number of the chunk\n",
    "for n, index_chunk in enumerate(tqdm(index)):\n",
    "    AIRS_CH0_clean = np.zeros((CHUNKS_SIZE, 11250, 32, l))\n",
    "    FGS1_clean = np.zeros((CHUNKS_SIZE, 135000, 32, 32))\n",
    "\n",
    "    for i in range(CHUNKS_SIZE):\n",
    "        df = pd.read_parquet(os.path.join(dir_name, f\"train/{index_chunk[i]}/AIRS-CH0_signal_0.parquet\"))\n",
    "\n",
    "        signal = df.values.astype(np.float64).reshape((df.shape[0], 32, 356))\n",
    "\n",
    "        signal = adc_airs(signal)\n",
    "\n",
    "        dt_airs = axis_info[\"AIRS-CH0-integration_time\"].dropna().values\n",
    "        dt_airs[1::2] += 0.1    # time integration - exposure time for an observation\n",
    "        chopped_signal = signal[:, :, cut_inf:cut_sup]   # trimmed signal pixels\n",
    "\n",
    "        del signal, df    # prevent memory buildup during chunk processing\n",
    "\n",
    "        # Cleaning the airs data (trimmming the wavelength pixels and changing the datatype of the values )\n",
    "\n",
    "        flat = pd.read_parquet(os.path.join(dir_name,f'train/{index_chunk[i]}/AIRS-CH0_calibration_0/flat.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "        dark = pd.read_parquet(os.path.join(dir_name,f'train/{index_chunk[i]}/AIRS-CH0_calibration_0/dark.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "        dead_airs = pd.read_parquet(os.path.join(dir_name,f'train/{index_chunk[i]}/AIRS-CH0_calibration_0/dead.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "        linear_corr = pd.read_parquet(os.path.join(dir_name,f'train/{index_chunk[i]}/AIRS-CH0_calibration_0/linear_corr.parquet')).values.astype(np.float64).reshape((6, 32, 356))[:, :, cut_inf:cut_sup]\n",
    "\n",
    "        if DO_MASK:    # masking dead and thermal pixels\n",
    "            chopped_signal = masking_dead_hot(chopped_signal, dead_airs, dark)\n",
    "            AIRS_CH0_clean[i] = chopped_signal\n",
    "        else:\n",
    "            AIRS_CH0_clean[i] = chopped_signal\n",
    "\n",
    "        if DO_THE_NL_CORR:    # correcting non-linearity of pixels \n",
    "            linear_corr_signal = apply_lin_corr(linear_corr, AIRS_CH0_clean[i])\n",
    "            AIRS_CH0_clean[i] = linear_corr_signal\n",
    "\n",
    "        if DO_DARK:    # subtracting the dark current noise \n",
    "            cleaned_signal = dark_current_sub(AIRS_CH0_clean[i], dead_airs, dark, dt_airs)\n",
    "            AIRS_CH0_clean[i] = cleaned_signal\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        del flat, dark, linear_corr, chopped_signal, dt_airs\n",
    "        # Cleaning the FGS1 data \n",
    "\n",
    "        df = pd.read_parquet(os.path.join(dir_name,f'train/{index_chunk[i]}/FGS1_signal_0.parquet'))\n",
    "        fgs_signal = df.values.astype(np.float64).reshape((df.shape[0], 32, 32))\n",
    "\n",
    "        fgs_signal = adc_fgs(fgs_signal)\n",
    "\n",
    "        dt_fgs1 = np.ones(len(fgs_signal))*0.1\n",
    "        dt_fgs1[1::2] += 0.1\n",
    "        chopped_FGS1 = fgs_signal.copy()  \n",
    "        del fgs_signal, df\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        # cleaning the data\n",
    "\n",
    "        flat = pd.read_parquet(os.path.join(dir_name,f'train/{index_chunk[i]}/FGS1_calibration_0/flat.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "        dark = pd.read_parquet(os.path.join(dir_name,f'train/{index_chunk[i]}/FGS1_calibration_0/dark.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "        dead_fgs1 = pd.read_parquet(os.path.join(dir_name,f'train/{index_chunk[i]}/FGS1_calibration_0/dead.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "        linear_corr = pd.read_parquet(os.path.join(dir_name,f'train/{index_chunk[i]}/FGS1_calibration_0/linear_corr.parquet')).values.astype(np.float64).reshape((6, 32, 32))\n",
    "\n",
    "        if DO_MASK:\n",
    "            chopped_FGS1 = masking_dead_hot(chopped_FGS1, dead_fgs1, dark)\n",
    "            FGS1_clean[i] = chopped_FGS1\n",
    "        else:\n",
    "            FGS1_clean[i] = chopped_FGS1\n",
    "\n",
    "        if DO_THE_NL_CORR: \n",
    "            linear_corr_signal = apply_lin_corr(linear_corr,FGS1_clean[i])\n",
    "            FGS1_clean[i,:, :, :] = linear_corr_signal\n",
    "        del linear_corr\n",
    "        gc.collect()\n",
    "        \n",
    "        if DO_DARK: \n",
    "            cleaned_signal = dark_current_sub(FGS1_clean[i], dead_fgs1, dark,dt_fgs1)\n",
    "            FGS1_clean[i] = cleaned_signal\n",
    "        del flat, dark, chopped_FGS1, dt_fgs1\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    # SAVE DATA AND FREE SPACE\n",
    "    AIRS_cds = get_cds(AIRS_CH0_clean)   # correlated double sampling\n",
    "    FGS1_cds = get_cds(FGS1_clean)\n",
    "    \n",
    "    del AIRS_CH0_clean, FGS1_clean\n",
    "    gc.collect()\n",
    "\n",
    "    # Time binning \n",
    "\n",
    "    if TIME_BINNING:\n",
    "        AIRS_cds_binned = bin_obs(30, AIRS_cds)\n",
    "        FGS1_cds_binned = bin_obs(30*12, FGS1_cds)\n",
    "    else:\n",
    "        AIRS_cds = AIRS_cds.transpose(0,1,3,2) ## this is important to make it consistent for flat fielding, but you can always change it\n",
    "        AIRS_cds_binned = AIRS_cds\n",
    "        FGS1_cds = FGS1_cds.transpose(0,1,3,2)\n",
    "        FGS1_cds_binned = FGS1_cds\n",
    "    \n",
    "    del AIRS_cds, FGS1_cds\n",
    "    gc.collect()\n",
    "\n",
    "    # Flat field correction \n",
    "\n",
    "    for i in range (CHUNKS_SIZE):\n",
    "        flat_airs = pd.read_parquet(os.path.join(dir_name,f'train/{index_chunk[i]}/AIRS-CH0_calibration_0/flat.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "        flat_fgs = pd.read_parquet(os.path.join(dir_name,f'train/{index_chunk[i]}/FGS1_calibration_0/flat.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "        if DO_FLAT:\n",
    "            corrected_AIRS_cds_binned = correct_flat_field(flat_airs,dead_airs, AIRS_cds_binned[i])\n",
    "            AIRS_cds_binned[i] = corrected_AIRS_cds_binned\n",
    "            corrected_FGS1_cds_binned = correct_flat_field(flat_fgs,dead_fgs1, FGS1_cds_binned[i])\n",
    "            FGS1_cds_binned[i] = corrected_FGS1_cds_binned\n",
    "        del flat_airs, flat_fgs\n",
    "        gc.collect()\n",
    "\n",
    "    # saving the data chunk by chunk\n",
    "    np.save(os.path.join(path_out, f\"AIRS_clean_train_{n}.npy\"), AIRS_cds_binned)\n",
    "    np.save(os.path.join(path_out, f\"FGS1_clean_train_{n}.npy\"), FGS1_cds_binned)\n",
    "\n",
    "    del AIRS_cds_binned\n",
    "    del FGS1_cds_binned\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb7f61",
   "metadata": {},
   "source": [
    "Concatenating all the dataset into one single datasheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396cef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file, chunk_size, nb_files):\n",
    "    data0 = np.load(file + \"_0.npy\")   # loading the first chunk\n",
    "    data_all = np.zeros((nb_files*chunk_size, data0.shape[1], data0.shape[2], data0.shape[3]))\n",
    "\n",
    "    data_all[:chunk_size] = data0    # imputing the first chunk values in the zeros array\n",
    "\n",
    "    for i in range(1, nb_files):   # loading all the chunks \n",
    "        data_all[i*chunk_size:(i+1)*chunk_size] = np.load(file + f'_{i}.npy')\n",
    "    return data_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e20331",
   "metadata": {},
   "source": [
    "Loading and saving the data imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8cc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_airs = load_data(path_out + \"AIRS_clean_train\", CHUNKS_SIZE, len(index))\n",
    "data_train_fgs1 = load_data(path_out + \"FGS1_clean_train\", CHUNKS_SIZE, len(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a761df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./\" + \"airs_data_train.npy\", data_train_airs)\n",
    "np.save(\"./\" + \"FGS1_data_train.npy\", data_train_fgs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cca547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data\n",
    "\n",
    "np.save(\"./\" + \"airs_data_train.npy\", data_train_airs)\n",
    "np.save(\"./\" + \"FGS1_data_train.npy\", data_train_fgs1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
